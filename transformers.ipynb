{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48c6600b",
   "metadata": {},
   "source": [
    "# Transformers Benchmarks\n",
    "\n",
    "Evaluate Bert/GPT on single/multi GPUs. \n",
    "\n",
    "Install libraries for our benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c9a07eb",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'transformers' already exists and is not an empty directory.\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Processing /workspace/transformers\n",
      "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers==4.22.0.dev0) (4.64.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers==4.22.0.dev0) (6.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers==4.22.0.dev0) (2.27.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers==4.22.0.dev0) (2022.6.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers==4.22.0.dev0) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers==4.22.0.dev0) (21.3)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 8.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers==4.22.0.dev0) (3.7.1)\n",
      "Collecting huggingface-hub<1.0,>=0.8.1\n",
      "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 78.7 MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.8.1->transformers==4.22.0.dev0) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers==4.22.0.dev0) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.22.0.dev0) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.22.0.dev0) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.22.0.dev0) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.22.0.dev0) (3.3)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.22.0.dev0-py3-none-any.whl size=4728127 sha256=b417ce2fe4e6d085edc5c67ca2d83e48eca02b20421dad927f1b15aea88d5633\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-jnsw40y1/wheels/0f/5c/f2/8c8d86e295e006136c70d83a43443060fb6cbbb43a286d4d02\n",
      "Successfully built transformers\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.8.1 tokenizers-0.12.1 transformers-4.22.0.dev0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n",
      "\u001b[K     |████████████████████████████████| 365 kB 6.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting evaluate\n",
      "  Downloading evaluate-0.2.2-py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 68.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting deepspeed\n",
      "  Downloading deepspeed-0.7.0.tar.gz (629 kB)\n",
      "\u001b[K     |████████████████████████████████| 629 kB 67.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets) (1.22.4)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 60.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[K     |████████████████████████████████| 212 kB 81.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.8.1)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.8/site-packages (from datasets) (2022.5.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.4.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets) (4.64.0)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.13-py38-none-any.whl (131 kB)\n",
      "\u001b[K     |████████████████████████████████| 131 kB 80.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dill<0.3.6\n",
      "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "\u001b[K     |████████████████████████████████| 95 kB 72.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2.27.1)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (7.0.0)\n",
      "Collecting hjson\n",
      "  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 59.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ninja\n",
      "  Downloading ninja-1.10.2.3-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (108 kB)\n",
      "\u001b[K     |████████████████████████████████| 108 kB 82.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from deepspeed) (5.9.1)\n",
      "Collecting py-cpuinfo\n",
      "  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
      "\u001b[K     |████████████████████████████████| 99 kB 79.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pydantic in /opt/conda/lib/python3.8/site-packages (from deepspeed) (1.8.2)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (from deepspeed) (1.13.0a0+08820cb)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.3.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.9)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (262 kB)\n",
      "\u001b[K     |████████████████████████████████| 262 kB 61.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
      "\u001b[K     |████████████████████████████████| 161 kB 61.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "\u001b[K     |████████████████████████████████| 121 kB 83.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Building wheels for collected packages: deepspeed, py-cpuinfo\n",
      "  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.7.0-py3-none-any.whl size=644048 sha256=71cc0e86bde6d2eebe579b0ba362d15ac0ea05d8ade2ed99c6397788c2b58dd4\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-vzrgtz95/wheels/89/b1/1f/36abd13839a2c71b019c76b220576c62c2b16fe0558dbd326c\n",
      "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22257 sha256=61953119568447c738043eca9a6d96cd88ecd28c8feb6361c2a10d630833277f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-vzrgtz95/wheels/57/cb/6d/bab2257f26c5be4a96ff65c3d2a7122c96529b73773ee37f36\n",
      "Successfully built deepspeed py-cpuinfo\n",
      "Installing collected packages: multidict, frozenlist, yarl, async-timeout, aiosignal, dill, aiohttp, xxhash, responses, multiprocess, py-cpuinfo, ninja, hjson, datasets, evaluate, deepspeed\n",
      "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 datasets-2.4.0 deepspeed-0.7.0 dill-0.3.5.1 evaluate-0.2.2 frozenlist-1.3.1 hjson-3.1.0 multidict-6.0.2 multiprocess-0.70.13 ninja-1.10.2.3 py-cpuinfo-8.0.0 responses-0.18.0 xxhash-3.0.0 yarl-1.8.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/transformers\n",
    "!cd transformers; pip install .\n",
    "!pip install datasets evaluate deepspeed psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d64d057",
   "metadata": {},
   "source": [
    "A few utility functions.\n",
    "\n",
    "The TFLOPS of a BERT-like or GPT-like model to train one example. We ignored vector operations such as LayerNorm and weight updates for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2f0db366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_tflops(num_layers, hidden_size, vocab_size, seq_len):\n",
    "    attention = 4 * hidden_size * seq_len**2 + 8 * seq_len * hidden_size**2 \n",
    "    ffn = 16 * seq_len * hidden_size**2\n",
    "    embedding = 2 * seq_len * hidden_size * vocab_size\n",
    "    forward = num_layers * (attention + ffn) + embedding\n",
    "    return 3 * forward / 1e12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db7fb26",
   "metadata": {},
   "source": [
    "Find the number of examples per second from Huggingface's training log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b909e637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def throughput(output):\n",
    "    for l in output:\n",
    "        if 'CUDA out of memory' in l:\n",
    "            print('Out of GPU memory, try a smaller batch size')\n",
    "            return 0\n",
    "        if '{\\'train_runtime' in l:\n",
    "            metrics = json.loads(l.replace('\\'', '\\\"'))\n",
    "            gpu_mem = metrics['init_mem_cpu_peaked_delta'] + \\\n",
    "                metrics['train_mem_gpu_alloc_delta'] + metrics['train_mem_gpu_peaked_delta']\n",
    "            print('Total used GPU memory:\\t%.1f GB'% (gpu_mem/1e9))\n",
    "            r = metrics['train_samples_per_second']\n",
    "            print('# samples per second:\\t%.1f' %r)\n",
    "            return r\n",
    "    print('Unknown error, print output to check')\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac25574",
   "metadata": {},
   "source": [
    "## Bert on a Single GPU\n",
    "\n",
    "Add your model here if not exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "56d8ee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spec = {\n",
    "    # https://huggingface.co/bert-large-uncased/blob/main/config.json\n",
    "    'bert-large-uncased' : {\n",
    "        'num_layers' : 24, 'vocab_size' : 30522, 'hidden_size' : 1024},\n",
    "    # https://huggingface.co/bert-base-cased/blob/main/config.json\n",
    "    'bert-base-cased' : {\n",
    "        'num_layers' : 12, 'vocab_size' : 28996, 'hidden_size' : 768}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d7134d",
   "metadata": {},
   "source": [
    "Use fine-tuning BERT for [text-classifcation](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification) as our workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "90d1baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"mrpc\"\n",
    "model = \"bert-large-uncased\"\n",
    "batch_size = 48\n",
    "seq_len = 128\n",
    "fp16 = True # default: False\n",
    "optim = \"adamw_apex_fused\"  # default: adamw_hf\n",
    "gradient_checkpointing = False # default: False\n",
    "gradient_accumulation_steps = 4 # default: 1\n",
    "\n",
    "cmd = f'''rm -rf /tmp/{task}; \\\n",
    "cd transformers/examples/pytorch/text-classification; \\\n",
    "python run_glue.py \\\n",
    "  --model_name_or_path {model} \\\n",
    "  --task_name {task} \\\n",
    "  --do_train \\\n",
    "  --max_seq_length {seq_len} \\\n",
    "  --per_device_train_batch_size {batch_size} \\\n",
    "  --learning_rate 2e-5 \\\n",
    "  --num_train_epochs 1 \\\n",
    "  --fp16 {fp16} \\\n",
    "  --optim {optim} \\\n",
    "  --gradient_accumulation_steps {gradient_accumulation_steps} \\\n",
    "  --gradient_checkpointing {gradient_checkpointing} \\\n",
    "  --output_dir /tmp/{task}/ \\\n",
    "  --skip_memory_metrics False \\\n",
    "'''\n",
    "\n",
    "output = !$cmd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb86a08",
   "metadata": {},
   "source": [
    "Get performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a57a7e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total used GPU memory:\t14.6 GB\n",
      "# samples per second:\t158.0\n",
      "Measured TFLOPs:\t41.2\n"
     ]
    }
   ],
   "source": [
    "tflops = model_tflops(seq_len=seq_len, **model_spec[model]) * throughput(output)\n",
    "print('Measured TFLOPs:\\t%.1f' % tflops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f05b7e",
   "metadata": {},
   "source": [
    "## GPT on a Single GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6e3c8db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spec.update({\n",
    "    'gpt2': { # https://huggingface.co/gpt2/blob/main/config.json\n",
    "        'num_layers' : 12, 'vocab_size' : 50257, 'hidden_size' : 768},\n",
    "    'gpt2-medium': { # https://huggingface.co/gpt2-medium/blob/main/config.json \n",
    "        'num_layers' : 24, 'vocab_size' : 50257, 'hidden_size' : 1024}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3d706960",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt2-medium\"\n",
    "batch_size = 4\n",
    "seq_len = 512\n",
    "fp16 = True # default: False\n",
    "optim = \"adamw_apex_fused\"  # default: adamw_hf\n",
    "gradient_checkpointing = False # default: False\n",
    "gradient_accumulation_steps = 4 # default: 1\n",
    "\n",
    "\n",
    "cmd = f'''rm -rf /tmp/clm; \\\n",
    "cd transformers/examples/pytorch/language-modeling; \\\n",
    "python run_clm.py \\\n",
    "    --model_name_or_path {model} \\\n",
    "    --dataset_name wikitext \\\n",
    "    --dataset_config_name wikitext-2-raw-v1 \\\n",
    "    --per_device_train_batch_size {batch_size} \\\n",
    "    --do_train \\\n",
    "    --block_size {seq_len} \\\n",
    "    --fp16 {fp16} \\\n",
    "    --optim {optim} \\\n",
    "    --gradient_accumulation_steps {gradient_accumulation_steps} \\\n",
    "    --gradient_checkpointing {gradient_checkpointing} \\\n",
    "    --skip_memory_metrics False \\\n",
    "    --max_steps 25 \\\n",
    "    --output_dir /tmp/clm\n",
    "'''\n",
    "\n",
    "output = ! $cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dbe33699",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total used GPU memory:\t14.8 GB\n",
      "# samples per second:\t15.5\n",
      "Measured TFLOPs:\t18.0\n"
     ]
    }
   ],
   "source": [
    "tflops = model_tflops(seq_len=seq_len, **model_spec[model]) * throughput(output)\n",
    "print('Measured TFLOPs:\\t%.1f' % tflops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6aaa41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
