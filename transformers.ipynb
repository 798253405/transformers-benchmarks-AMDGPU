{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48c6600b",
   "metadata": {},
   "source": [
    "# Transformers Benchmarks\n",
    "\n",
    "Evaluate Bert and GPT training performance on single/multi GPUs. \n",
    "\n",
    "List all available GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10cbfc5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version\t: 1.13.0a0+08820cb\n",
      "CUDA version\t: 11.7\n",
      "GPU0\t\t: Tesla V100-SXM2-16GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print('Pytorch version\\t:', torch.__version__)\n",
    "print('CUDA version\\t:', torch.version.cuda)\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f'GPU{i}\\t\\t:',torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3afb5c",
   "metadata": {},
   "source": [
    "## Installation and Utilities\n",
    "\n",
    "Install huggingface and deepspeed. Note that `transformers` is installed from source to run its examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c9a07eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "!git clone https://github.com/huggingface/transformers\n",
    "!cd transformers; pip install .\n",
    "!pip install datasets evaluate accelerate deepspeed psutil\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db7fb26",
   "metadata": {},
   "source": [
    "Get the model specification given its name in [Huggingface Hub](https://huggingface.co/models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe4aa622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass, asdict\n",
    "from transformers import AutoConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8274af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    model: str         # huggingface model name\n",
    "    seq_len: int       # input sequence length\n",
    "    batch_size: int    # batch size per GPU\n",
    "        \n",
    "    ## Improve speed / reduce memory  \n",
    "    bf16: bool = False  # Faster, less memory. Recommend if GPU supports\n",
    "    fp16: bool = False  # Faster, less memory, but need to scale loos. \n",
    "                        # Recommend if BF16 is not available.\n",
    "    optim: str = 'adamw_hf'  # Optimization method\n",
    "    grad_ckpt: bool = False  # save memory with an extra forward\n",
    "    grad_accum: int = 1      # accumulate gradients for better performance\n",
    "    steps: int = 50          # number of batches to benchmark\n",
    "        \n",
    "    ## Multi-GPUs\n",
    "    gpus: str = '0'          # GPUs to use. \"0,1\" means use GPU 0 and 1\n",
    "    ddp: bool = False        # if or not use pytorch's DistributedDataParallel\n",
    "    deepspeed: bool = False  # if or not use deepspeed\n",
    "    ds_config: str = ''      # deepspeed config \n",
    "    \n",
    "    def TFLOPs(self):\n",
    "        \"\"\"Tera floating points operators to train one example\"\"\"\n",
    "        spec = AutoConfig.from_pretrained(self.model)\n",
    "        get = lambda *keys: max(\n",
    "            [getattr(spec, k) if hasattr(spec, k) else 0 for k in keys])\n",
    "        n = get('num_hidden_layers', 'n_layer')\n",
    "        h = get('hidden_size', 'n_embd', 'd_model')\n",
    "        s = self.seq_len\n",
    "        v = get('vocab_size')\n",
    "        att, ffn, embed = 4*h*s**2 + 8*s*h**2, 16*s*h**2, 2*s*h*v\n",
    "        forward = n*(att+ffn) + embed\n",
    "        return (4 * forward if self.grad_ckpt else 3 * forward) / 1e12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75c539a",
   "metadata": {},
   "source": [
    "Parse Huggingface to get GPU memory consumption and speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4793ee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_summary(config, log_filename):\n",
    "    with open(log_filename) as f:\n",
    "        lines = f.readlines()\n",
    "    for l in lines:\n",
    "        if 'CUDA out of memory' in l:\n",
    "            print('Out of GPU memory, try a smaller batch size')\n",
    "            return\n",
    "        if '{\\'train_runtime' in l:\n",
    "            metrics = json.loads(l.replace('\\'', '\\\"'))\n",
    "            gpu_mem = metrics['init_mem_cpu_peaked_delta'] + \\\n",
    "                    metrics['train_mem_gpu_alloc_delta'] + metrics['train_mem_gpu_peaked_delta']\n",
    "            r = metrics['train_samples_per_second']\n",
    "            num_gpus = len(config.gpus.split(','))\n",
    "            print('Total samples / second\\t: %.1f' % r)\n",
    "            print('Per GPU memory (GB)\\t: %.1f'% (gpu_mem/1e9))\n",
    "            print('Per GPU TFLOPs\\t\\t: %.1f' % (r * config.tflops() / num_gpus))\n",
    "            return\n",
    "    print(f'Failed. Check \"{log_filename}\" to find error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1ba310",
   "metadata": {},
   "source": [
    "Get the launcher based on config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69cab006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def launcher(config):\n",
    "    if config.ddp:\n",
    "        num_gpus = len(config.gpus.split(','))\n",
    "        return f'python -m torch.distributed.launch --nproc_per_node {num_gpus}'\n",
    "    return 'deepspeed' if config.deepspeed else 'python'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac25574",
   "metadata": {},
   "source": [
    "## Bert on a Single GPU\n",
    "\n",
    "We use the [masked langunage modeling](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling) task as the benchmark workload. It's a good proximation of BERT pre-training, but no needs to prepare the dataset.\n",
    "\n",
    "The following function maps our configure into a command for `run_mlm.py`. The log is saved into `log.txt`, and we print a training summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75817d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python transformers/examples/pytorch/language-modeling/run_mlm.py --help >help.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59a16466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bert(config):\n",
    "    cmd = f'''rm -rf /tmp/bert; \\\n",
    "export CUDA_VISIBLE_DEVICES={config.gpus}; \\\n",
    "{launcher(config)} transformers/examples/pytorch/language-modeling/run_mlm.py \\\n",
    "  --config_name {config.model} \\\n",
    "  --tokenizer_name {config.model} \\\n",
    "  --dataset_name wikitext \\\n",
    "  --dataset_config_name wikitext-2-raw-v1 \\\n",
    "  --do_train \\\n",
    "  --max_seq_length {config.seq_len} \\\n",
    "  --per_device_train_batch_size {config.batch_size} \\\n",
    "  --fp16 {config.fp16} \\\n",
    "  --bf16 {config.bf16} \\\n",
    "  --optim {config.optim} \\\n",
    "  --gradient_accumulation_steps {config.grad_accum} \\\n",
    "  --gradient_checkpointing {config.grad_ckpt} \\\n",
    "  --max_steps {config.steps} \\\n",
    "  --output_dir /tmp/bert/ \\\n",
    "  --skip_memory_metrics False'''\n",
    "    if config.deepspeed:\n",
    "        cmd += f' --deepspeed {config.ds_config}'\n",
    "    cmd += ' > log.txt 2>&1'\n",
    "    os.system(cmd)\n",
    "    log_summary(config, 'log.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69d8688",
   "metadata": {},
   "source": [
    "We use a 128 sequence length, which is used in 90\\% steps in the original BERT paper. Then choose a large batch size that will not cause out of memory for a good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82031e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of GPU memory, try a smaller batch size\n"
     ]
    }
   ],
   "source": [
    "bert_1 = Config('bert-large-uncased', 128, 56)\n",
    "run_bert(bert_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf7b183",
   "metadata": {},
   "source": [
    "Now switch to `bf16`. Use `fp16=True` if your GPU architecture is before Ampere. But we recommend you to use `bf16` if available. It doesn't require to tune the [loss scaling](https://moocaholic.medium.com/fp64-fp32-fp16-bfloat16-tf32-and-other-members-of-the-zoo-a1ca7897d407), due to more exponent bits compared to `fp16`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19ec9a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples / second\t: 135.3\n",
      "Per GPU memory (GB)\t: 18.2\n",
      "Per GPU TFLOPs\t\t: 35.3\n"
     ]
    }
   ],
   "source": [
    "bert_2 = Config('bert-large-uncased', 128, 56, bf16=True)\n",
    "run_bert(bert_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa998555",
   "metadata": {},
   "source": [
    "You can see both an improved performance and a reduction of memory usage. The speed improvement is due to using Tensor Cores.\n",
    "\n",
    "The memory usage is mainly due to three parts: model parameters, layer outputs in the forward path (activations) and workspace memory used by backend libraries.  It may surprise you that neither `fp16` or `bf16` save memory related to model parameters. The reason is because model updating is running with 32-bit. For one model parameter: \n",
    "\n",
    "- with normal `fp32`, we use 4 bytes for the 32-bit weight, 4 bytes for the 32-bit gradient, 8 bytes for the two momentums in Adam, a total of 16 bytes\n",
    "- with `fp16` or `bf16`, we use 2 bytes for the 16-bit weight, 2 bytes for the 16-bit gradient (some implementation uses 32-bit gradient), 4 bytes for the master 32-bit weight, and 8 bytes for the two momentums in adam, with a total of 16 bytes \n",
    "\n",
    "The memory saving contributes to all activations are stored in 16-bit now. The activation size is linear to the batch size, sequence length, number of layers and hidden size. If you spend a lot of memory for activation, then both `bf16` or `fp16` will help a lot. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e11a844",
   "metadata": {},
   "source": [
    "Now we can use a larger batch size, which further improves performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dae8303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples / second\t: 144.9\n",
      "Per GPU memory (GB)\t: 23.4\n",
      "Per GPU TFLOPs\t\t: 37.8\n"
     ]
    }
   ],
   "source": [
    "bert_3 = Config('bert-large-uncased', 128, 80, bf16=True)\n",
    "run_bert(bert_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd12e96",
   "metadata": {},
   "source": [
    "The model updating involves multiple vector operators. It causes unignorable overheads. Replacing it with a better implementation helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d972b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples / second\t: 152.5\n",
      "Per GPU memory (GB)\t: 23.4\n",
      "Per GPU TFLOPs\t\t: 39.8\n"
     ]
    }
   ],
   "source": [
    "bert_4 = Config('bert-large-uncased', 128, 80, bf16=True, optim='adamw_apex_fused')\n",
    "run_bert(bert_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b12ca2",
   "metadata": {},
   "source": [
    "To further reduce the optimization overhead, we can accumulate the gradients multiple times before updating weight. If we accumulate 4 times, then it leads to an 4x larger effective batch size. It may be too big for the fine tuning task, but not a problem for pre-training. Also note that this option needs extra buff for gradients, so it may require you to use a smaller batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0bc0213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples / second\t: 157.2\n",
      "Per GPU memory (GB)\t: 22.6\n",
      "Per GPU TFLOPs\t\t: 41.0\n"
     ]
    }
   ],
   "source": [
    "bert_5 = Config('bert-large-uncased', 128, 76, bf16=True, optim='adamw_apex_fused', \n",
    "                grad_accum=4, steps=10)\n",
    "run_bert(bert_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7110f1",
   "metadata": {},
   "source": [
    "To further improve batch size, we can throw away activations, and re-compute them when needed. Now we can use a near 4x larger batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93b1415f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples / second\t: 138.9\n",
      "Per GPU memory (GB)\t: 19.4\n",
      "Per GPU TFLOPs\t\t: 48.3\n"
     ]
    }
   ],
   "source": [
    "bert_6 = Config('bert-large-uncased', 128, 260, bf16=True, optim='adamw_apex_fused', \n",
    "                grad_accum=4, grad_ckpt=True, steps=5)\n",
    "run_bert(bert_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deff3e8",
   "metadata": {},
   "source": [
    "Though it furthers improve TFLOPS, but decreases the number of samples per second because of the extra forward. So use it only when the model is too big you cannot use an effective batch size. \n",
    "\n",
    "So the best option is `bert_5`. You can save it as plain text for sharing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e5512a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'bert-large-uncased',\n",
       " 'seq_len': 128,\n",
       " 'batch_size': 76,\n",
       " 'bf16': True,\n",
       " 'fp16': False,\n",
       " 'optim': 'adamw_apex_fused',\n",
       " 'grad_ckpt': False,\n",
       " 'grad_accum': 4,\n",
       " 'steps': 10,\n",
       " 'gpus': '0',\n",
       " 'ddp': False,\n",
       " 'deepspeed': False,\n",
       " 'ds_config': ''}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asdict(bert_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f05b7e",
   "metadata": {},
   "source": [
    "## GPT-2 on a Single GPU\n",
    "\n",
    "Next we train language model with GPT-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c6e62cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gpt(config):\n",
    "    cmd = f'''rm -rf /tmp/gpt; \\\n",
    "export CUDA_VISIBLE_DEVICES={config.gpus}; \\\n",
    "{launcher(config)} transformers/examples/pytorch/language-modeling/run_clm.py \\\n",
    "  --model_name_or_path {config.model} \\\n",
    "  --dataset_name wikitext \\\n",
    "  --dataset_config_name wikitext-2-raw-v1 \\\n",
    "  --do_train \\\n",
    "  --per_device_train_batch_size {config.batch_size} \\\n",
    "  --block_size {config.seq_len} \\\n",
    "  --learning_rate 2e-5 \\\n",
    "  --max_steps {config.steps} \\\n",
    "  --fp16 {config.fp16} \\\n",
    "  --bf16 {config.bf16} \\\n",
    "  --optim {config.optim} \\\n",
    "  --gradient_accumulation_steps {config.grad_accum} \\\n",
    "  --gradient_checkpointing {config.grad_ckpt} \\\n",
    "  --output_dir /tmp/gpt/ \\\n",
    "  --skip_memory_metrics False'''\n",
    "    if config.deepspeed:\n",
    "        cmd += f' --deepspeed {config.ds_config}'\n",
    "    cmd += ' > log.txt 2>&1'\n",
    "    os.system(cmd)\n",
    "    log_summary(config, 'log.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6212ad0",
   "metadata": {},
   "source": [
    "We use `gpt2-medium` whose architecture is similar to `bert-large`. GPT-2 models uses a larger sequence length, here we pick 512, which leads to a much smaller batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "243f7428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples / second\t: 13.6\n",
      "Per GPU memory (GB)\t: 22.4\n",
      "Per GPU TFLOPs\t\t: 15.8\n"
     ]
    }
   ],
   "source": [
    "gpt_1 = Config(\"gpt2-medium\", 512, 6)\n",
    "run_gpt(gpt_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0e94fa",
   "metadata": {},
   "source": [
    "Use a configure similar to `bert_5`. But note that the batch size increase is smaller than BERT when using `bf16`. Also we observed a smaller TFLOPS (24.6 vs 41). The reason is unknown to us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b4a163c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples / second\t: 21.1\n",
      "Per GPU memory (GB)\t: 21.2\n",
      "Per GPU TFLOPs\t\t: 24.6\n"
     ]
    }
   ],
   "source": [
    "gpt_2 = Config(\"gpt2-medium\", 512, 7, bf16=True, optim='adamw_apex_fused', \n",
    "                grad_accum=4)\n",
    "run_gpt(gpt_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8eef1b8",
   "metadata": {},
   "source": [
    "## Multiple GPUs\n",
    "\n",
    "Huggingface uses multiple GPUs with data parallelism when multi-GPUs are available. Here we train on two GPUs based on configure `bert_5`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99524781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples / second\t: 255.4\n",
      "Per GPU memory (GB)\t: 23.1\n",
      "Per GPU TFLOPs\t\t: 33.3\n"
     ]
    }
   ],
   "source": [
    "mbert_1 = Config('bert-large-uncased', 128, 76, bf16=True, optim='adamw_apex_fused', \n",
    "                grad_accum=4, steps=10, gpus='0,1')\n",
    "run_bert(mbert_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99073153",
   "metadata": {},
   "source": [
    "You can see the per GPU TFLOPs is reduced due to communication overhead. Using Pytorch DistributedDataParallel helps. But note that it uses extra memory for communication, so we need to reduce the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c36556b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples / second\t: 316.2\n",
      "Per GPU memory (GB)\t: 22.7\n",
      "Per GPU TFLOPs\t\t: 41.2\n"
     ]
    }
   ],
   "source": [
    "mbert_2 = mbert_1\n",
    "mbert_2.ddp = True\n",
    "mbert_2.batch_size = 70\n",
    "run_bert(mbert_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0db5e4b",
   "metadata": {},
   "source": [
    "GPUs are connected by NVLinks. Let's test the speed without using NVLinks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a641c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\u001b[4mGPU0\tGPU1\tCPU Affinity\tNUMA Affinity\u001b[0m\r\n",
      "GPU0\t X \tNV4\t0-23\t\tN/A\r\n",
      "GPU1\tNV4\t X \t0-23\t\tN/A\r\n",
      "\r\n",
      "Legend:\r\n",
      "\r\n",
      "  X    = Self\r\n",
      "  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n",
      "  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n",
      "  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n",
      "  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n",
      "  PIX  = Connection traversing at most a single PCIe bridge\r\n",
      "  NV#  = Connection traversing a bonded set of # NVLinks\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi topo -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "574b55d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples / second\t: 305.9\n",
      "Per GPU memory (GB)\t: 22.7\n",
      "Per GPU TFLOPs\t\t: 39.9\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "run_bert(mbert_2)\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d77fbf9",
   "metadata": {},
   "source": [
    "The performance is only slightly decreased. The reason is we are using a relative batch size, and accumulate gradients by 4 times, so the weight updating cost is small compared to others.\n",
    "\n",
    "Next let's use DeepSpeed with Zero 2, which has a worse performance compared to DDP, but allow to use a larger batch size as model and optimizer status are partitioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd7f6ec3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples / second\t: 297.6\n",
      "Per GPU memory (GB)\t: 22.4\n",
      "Per GPU TFLOPs\t\t: 38.8\n"
     ]
    }
   ],
   "source": [
    "mbert_3 = mbert_1\n",
    "mbert_3.deepspeed = True\n",
    "mbert_3.ds_config = 'transformers/tests/deepspeed/ds_config_zero2.json'\n",
    "mbert_3.batch_size = 128\n",
    "run_bert(mbert_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dbc4a1",
   "metadata": {},
   "source": [
    "Lastly let's test GPT-2, using its default 1024 sequence length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "676dbe19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples / second\t: 7.0\n",
      "Per GPU memory (GB)\t: 16.2\n",
      "Per GPU TFLOPs\t\t: 18.8\n"
     ]
    }
   ],
   "source": [
    "mgpt_1 = Config(\"gpt2-large\", 1024, 2, bf16=True, optim='adamw_apex_fused', \n",
    "                grad_accum=16, gpus='0,1', steps=5, deepspeed=True, \n",
    "                ds_config='transformers/tests/deepspeed/ds_config_zero2.json')\n",
    "run_gpt(mgpt_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554bb727",
   "metadata": {},
   "source": [
    "The performance is degraded more when NVLinks are not available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "879f1242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples / second\t: 5.6\n",
      "Per GPU memory (GB)\t: 16.2\n",
      "Per GPU TFLOPs\t\t: 15.0\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "run_gpt(mgpt_1)\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34b5bf5",
   "metadata": {},
   "source": [
    "Here is the screen shot of `nvtop` when executing the above cell. GPUs are idle when communication. \n",
    "\n",
    "![](imgs/nvtop.png)\n",
    "\n",
    "Finally, let's train the 1.3B GPT-2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a19505fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples / second\t: 2.2\n",
      "Per GPU memory (GB)\t: 15.3\n",
      "Per GPU TFLOPs\t\t: 11.8\n"
     ]
    }
   ],
   "source": [
    "mgpt_2 = Config(\"gpt2-xl\", 1024, 1, bf16=True, optim='adamw_apex_fused', \n",
    "                grad_accum=16, gpus='0,1', deepspeed=True, steps=5,\n",
    "                ds_config='transformers/tests/deepspeed/ds_config_zero2.json')\n",
    "run_gpt(mgpt_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a8f6f9",
   "metadata": {},
   "source": [
    "Duo the GPU memory size, we can only compute one example per time, even when using Zero-2. It leads to unsatisfied performance.\n",
    "\n",
    "## Discussions\n",
    "\n",
    "We explore several options to tune Huggingface's example code and understand their performance. But note HF have other flags that may further improve performance. Also there are other libraries reported higher performance. We will discuss more options in other notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
