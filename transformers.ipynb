{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48c6600b",
   "metadata": {},
   "source": [
    "# Transformers Benchmarks\n",
    "\n",
    "Evaluate Bert GPT training performance on single/multi GPUs. \n",
    "\n",
    "## Installation and Utilities\n",
    "\n",
    "Install huggingface and deepspeed. Note that `transformers` is installed from source, as we will run its examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c9a07eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "!git clone https://github.com/huggingface/transformers\n",
    "!cd transformers; pip install .\n",
    "!pip install datasets evaluate accelerate deepspeed psutil\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db7fb26",
   "metadata": {},
   "source": [
    "Get the model specification given its name in [Huggingface Hub](https://huggingface.co/models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe4aa622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelSpec(num_layers=24, hidden_size=1024, vocab_size=30522)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass, asdict\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "@dataclass\n",
    "class ModelSpec:\n",
    "    num_layers: int  # number of Transformer blocks\n",
    "    hidden_size: int # attention/ffn dimensions\n",
    "    vocab_size: int  # vocabulary size\n",
    "        \n",
    "    @staticmethod\n",
    "    def from_config(model_name: str):\n",
    "        page = requests.get(f'https://huggingface.co/{model_name}/raw/main/config.json')\n",
    "        assert page.ok, f'failed to get the config of {model_name}'\n",
    "        spec = page.json()\n",
    "        get = lambda *keys: max(int(spec.get(k, 0)) for k in keys)\n",
    "        num_layers = get('num_hidden_layers', 'n_layer')\n",
    "        hidden_size = get('hidden_size', 'n_embd')\n",
    "        vocab_size = get('vocab_size')\n",
    "        return ModelSpec(num_layers, hidden_size, vocab_size)\n",
    "    \n",
    "ModelSpec.from_config('bert-large-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5366996",
   "metadata": {},
   "source": [
    "Experiment configurations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8274af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    model: str         # huggingface model name\n",
    "    seq_len: int       # input sequence length\n",
    "    batch_size: int    # batch size per GPU\n",
    "        \n",
    "    ## Improve speed / reduce memory  \n",
    "    bf16: bool = False  # Faster, less memory. Recommend if GPU supports\n",
    "    fp16: bool = False  # Faster, less memory, but need to scale loos. \n",
    "                        # Recommend if BF16 is not available.\n",
    "    optim: str = 'adamw_hf'  # Optimization method\n",
    "    grad_ckpt: bool = False  # save memory with an extra forward\n",
    "    grad_accum: int = 1      # accumulate gradients for better performance\n",
    "    steps: int = 50          # number of batches to benchmark\n",
    "        \n",
    "    ## Multi-GPUs\n",
    "    gpus: str = '0'          # GPUs to use. \"0,1\" means use GPU 0 and 1\n",
    "    ddp: bool = False        # if or not use pytorch's DistributedDataParallel\n",
    "    deepspeed: bool = False  # if or not use deepspeed\n",
    "    ds_config: str = ''      # deepspeed config \n",
    "    \n",
    "    def tflops(self):\n",
    "        \"\"\"TeraFLOPS for training one example\"\"\"\n",
    "        # Ignored all vector operators for simplicity. \n",
    "        spec = ModelSpec.from_config(self.model) \n",
    "        attention = 4 * spec.hidden_size * self.seq_len**2 + \\\n",
    "                    8 * self.seq_len * spec.hidden_size**2 \n",
    "        ffn = 16 * self.seq_len * spec.hidden_size**2\n",
    "        embedding = 2 * self.seq_len * spec.hidden_size * spec.vocab_size\n",
    "        forward = spec.num_layers * (attention + ffn) + embedding\n",
    "        return (4 * forward if self.grad_ckpt else 3 * forward) / 1e12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75c539a",
   "metadata": {},
   "source": [
    "Parse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4793ee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_summary(config, log_filename):\n",
    "    with open(log_filename) as f:\n",
    "        lines = f.readlines()\n",
    "    for l in lines:\n",
    "        if 'CUDA out of memory' in l:\n",
    "            print('Out of GPU memory, try a smaller batch size')\n",
    "            return\n",
    "        if '{\\'train_runtime' in l:\n",
    "            metrics = json.loads(l.replace('\\'', '\\\"'))\n",
    "            gpu_mem = metrics['init_mem_cpu_peaked_delta'] + \\\n",
    "                    metrics['train_mem_gpu_alloc_delta'] + metrics['train_mem_gpu_peaked_delta']\n",
    "            print('Total used GPU memory:\\t%.1f GB'% (gpu_mem/1e9))\n",
    "            r = metrics['train_samples_per_second']\n",
    "            print('# samples per second:\\t%.1f' % r)\n",
    "            num_gpus = len(config.gpus.split(','))\n",
    "            print('Measured per GPU TFLOPs:\\t%.1f' % (r * config.tflops() / num_gpus))\n",
    "            return\n",
    "    print(f'Failed. Check \"{log_filename}\" to find error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69cab006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def launcher(config):\n",
    "    if config.ddp:\n",
    "        num_gpus = len(config.gpus.split(','))\n",
    "        return f'python -m torch.distributed.launch --nproc_per_node {num_gpus}'\n",
    "    if config.deepspeed:\n",
    "        return 'deepspeed'\n",
    "    return 'python'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac25574",
   "metadata": {},
   "source": [
    "## Bert on a Single GPU\n",
    "\n",
    "Even though we are interested in pre-training, we \n",
    "use fine-tuning BERT for [text-classifcation](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification) as a proximation, which only needs to download a small dataset.\n",
    "\n",
    "The follwing function maps our configure into a command for `run_glue.py`. The log is saved into `log.txt`, then we print a summary of the GPU memory usage and measured TFLOPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59a16466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bert(config):\n",
    "    cmd = f'''rm -rf /tmp/bert; \\\n",
    "export CUDA_VISIBLE_DEVICES={config.gpus}; \\\n",
    "{launcher(config)} transformers/examples/pytorch/language-modeling/run_mlm.py \\\n",
    "  --model_name_or_path {config.model} \\\n",
    "  --dataset_name wikitext \\\n",
    "  --dataset_config_name wikitext-2-raw-v1 \\\n",
    "  --do_train \\\n",
    "  --max_seq_length {config.seq_len} \\\n",
    "  --per_device_train_batch_size {config.batch_size} \\\n",
    "  --fp16 {config.fp16} \\\n",
    "  --bf16 {config.bf16} \\\n",
    "  --optim {config.optim} \\\n",
    "  --gradient_accumulation_steps {config.grad_accum} \\\n",
    "  --gradient_checkpointing {config.grad_ckpt} \\\n",
    "  --max_steps {config.steps} \\\n",
    "  --output_dir /tmp/bert/ \\\n",
    "  --skip_memory_metrics False'''\n",
    "    if config.deepspeed:\n",
    "        cmd += f' --deepspeed {config.ds_config}'\n",
    "    cmd += ' > log.txt 2>&1'\n",
    "    os.system(cmd)\n",
    "    log_summary(config, 'log.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69d8688",
   "metadata": {},
   "source": [
    "Use a large batch size that will not cause out of memory for a good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82031e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total used GPU memory:\t23.0 GB\n",
      "# samples per second:\t88.9\n",
      "Measured TFLOPs:\t23.2\n"
     ]
    }
   ],
   "source": [
    "bert_1 = Config('bert-large-uncased', 128, 56)\n",
    "run_bert(bert_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf7b183",
   "metadata": {},
   "source": [
    "Now switch to `bf16`. You can see both an improved performance and a reduction of memory usage. The former is due to using Tensor Cores in new Nvidia GPUs. If you see an error or no improvement, please try `fp16=True`. We recommend you to use `bf16` as it doesn't require to tune the [loss scaling](https://moocaholic.medium.com/fp64-fp32-fp16-bfloat16-tf32-and-other-members-of-the-zoo-a1ca7897d407), due to more exponent bits compared to `fp16`. \n",
    "\n",
    "The memory usage is mainly due to three parts: model parameters, layout outputs in the forward path (activations) and workspace memory used by backend libraries.  It may surprise you that neither `fp16` or `bf16` save space related to model parameters. For one model parameter: \n",
    "\n",
    "- Use normal `fp32`, we use 4 bytes for the 32-bit weight, 4 bytes for the 32-bit gradient, 8 bytes for the two momentums in Adam, with a total of 16 bytes\n",
    "- Use `fp16` or `bf16`, we use 2 bytes for the 16-bit weight, 2 bytes for the 16-bit gradient (some implementation uses 32-bit gradient), 4 bytes for the master 32-bit weight, and 8 bytes for the two momentums in adam, with a total of 16 bytes \n",
    "\n",
    "The memory saving to mainly due to all activations are stored in 16-bit instead of 32-bit. The activation size is linear to the batch size and sequence length, so if your GPU memory is large, or model is small (or use zero to shard the model), then you save more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19ec9a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total used GPU memory:\t18.2 GB\n",
      "# samples per second:\t133.5\n",
      "Measured TFLOPs:\t34.8\n"
     ]
    }
   ],
   "source": [
    "bert_2 = Config('bert-large-uncased', 128, 56, bf16=True)\n",
    "run_bert(bert_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e11a844",
   "metadata": {},
   "source": [
    "Now we can use a larger batch size, which further improves performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5dae8303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total used GPU memory:\t23.4 GB\n",
      "# samples per second:\t145.0\n",
      "Measured TFLOPs:\t37.8\n"
     ]
    }
   ],
   "source": [
    "bert_3 = Config('bert-large-uncased', 128, 80, bf16=True)\n",
    "run_bert(bert_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd12e96",
   "metadata": {},
   "source": [
    "The model updating involes multiple vector operators. It causes unignorable overheads. Replacing it with a better implementation helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d972b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total used GPU memory:\t23.4 GB\n",
      "# samples per second:\t154.1\n",
      "Measured TFLOPs:\t40.2\n"
     ]
    }
   ],
   "source": [
    "bert_4 = Config('bert-large-uncased', 128, 80, bf16=True, optim='adamw_apex_fused')\n",
    "run_bert(bert_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b12ca2",
   "metadata": {},
   "source": [
    "To further reduce the optmization overhead, we can accumulate the gradients multiple times before updating weight. If we accumulate 4 times, then it leads to an effective 4\\*96 batch size. It may be too big for the fine tuning task, but not a problem for pre-training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0bc0213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total used GPU memory:\t22.6 GB\n",
      "# samples per second:\t159.7\n",
      "Measured TFLOPs:\t41.6\n"
     ]
    }
   ],
   "source": [
    "bert_5 = Config('bert-large-uncased', 128, 76, bf16=True, optim='adamw_apex_fused', \n",
    "                grad_accum=4, steps=10)\n",
    "run_bert(bert_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7110f1",
   "metadata": {},
   "source": [
    "To further improve batch size, we can throw away activations, and re-compute them when needed. Now we can use a 9x larger batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "93b1415f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total used GPU memory:\t19.2 GB\n",
      "# samples per second:\t139.2\n",
      "Measured TFLOPs:\t48.4\n"
     ]
    }
   ],
   "source": [
    "bert_6 = Config('bert-large-uncased', 128, 260, bf16=True, optim='adamw_apex_fused', \n",
    "                grad_accum=4, grad_ckpt=True, steps=5)\n",
    "run_bert(bert_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deff3e8",
   "metadata": {},
   "source": [
    "Though it furthers improve TFLOPS, but decreases the number of samples per second because of the extra forward. So use it only when the model is too big you cannot use an effective batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e5512a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'bert-large-uncased',\n",
       " 'batch_size': 96,\n",
       " 'seq_len': 128,\n",
       " 'bf16': True,\n",
       " 'fp16': False,\n",
       " 'optim': 'adamw_apex_fused',\n",
       " 'grad_ckpt': False,\n",
       " 'grad_accum': 4,\n",
       " 'gpus': '0',\n",
       " 'ddp': False,\n",
       " 'deepspeed': False,\n",
       " 'ds_config': ''}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asdict(bert_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f05b7e",
   "metadata": {},
   "source": [
    "## GPT-2 on a Single GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8c6e62cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gpt(config):\n",
    "    cmd = f'''rm -rf /tmp/gpt; \\\n",
    "export CUDA_VISIBLE_DEVICES={config.gpus}; \\\n",
    "{launcher(config)} transformers/examples/pytorch/language-modeling/run_clm.py \\\n",
    "  --model_name_or_path {config.model} \\\n",
    "  --dataset_name wikitext \\\n",
    "  --dataset_config_name wikitext-2-raw-v1 \\\n",
    "  --do_train \\\n",
    "  --per_device_train_batch_size {config.batch_size} \\\n",
    "  --block_size {config.seq_len} \\\n",
    "  --learning_rate 2e-5 \\\n",
    "  --max_steps {config.steps} \\\n",
    "  --fp16 {config.fp16} \\\n",
    "  --bf16 {config.bf16} \\\n",
    "  --optim {config.optim} \\\n",
    "  --gradient_accumulation_steps {config.grad_accum} \\\n",
    "  --gradient_checkpointing {config.grad_ckpt} \\\n",
    "  --output_dir /tmp/gpt/ \\\n",
    "  --skip_memory_metrics False'''\n",
    "    if config.deepspeed:\n",
    "        cmd += f' --deepspeed {config.ds_config}'\n",
    "    cmd += ' > log.txt 2>&1'\n",
    "    os.system(cmd)\n",
    "    log_summary(config, 'log.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "243f7428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total used GPU memory:\t22.4 GB\n",
      "# samples per second:\t13.5\n",
      "Measured TFLOPs:\t15.7\n"
     ]
    }
   ],
   "source": [
    "gpt_1 = Config(\"gpt2-medium\", 512, 6)\n",
    "run_gpt(gpt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1b4a163c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total used GPU memory:\t21.2 GB\n",
      "# samples per second:\t21.1\n",
      "Measured TFLOPs:\t24.5\n"
     ]
    }
   ],
   "source": [
    "gpt_2 = Config(\"gpt2-medium\", 512, 7, bf16=True, optim='adamw_apex_fused', \n",
    "                grad_accum=4)\n",
    "run_gpt(gpt_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8eef1b8",
   "metadata": {},
   "source": [
    "## Multiple GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "99524781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total used GPU memory:\t23.1 GB\n",
      "# samples per second:\t252.2\n",
      "Measured per GPU TFLOPs:\t32.9\n"
     ]
    }
   ],
   "source": [
    "mbert_1 = Config('bert-large-uncased', 128, 76, bf16=True, optim='adamw_apex_fused', \n",
    "                grad_accum=4, steps=10, gpus='0,1')\n",
    "run_bert(mbert_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7c36556b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total used GPU memory:\t22.7 GB\n",
      "# samples per second:\t319.9\n",
      "Measured per GPU TFLOPs:\t41.7\n"
     ]
    }
   ],
   "source": [
    "mbert_2 = mbert_1\n",
    "mbert_2.ddp = True\n",
    "mbert_2.batch_size = 70\n",
    "run_bert(mbert_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "574b55d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total used GPU memory:\t22.7 GB\n",
      "# samples per second:\t310.0\n",
      "Measured per GPU TFLOPs:\t40.4\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "run_bert(mbert_2)\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d77fbf9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fd7f6ec3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total used GPU memory:\t22.4 GB\n",
      "# samples per second:\t297.4\n",
      "Measured per GPU TFLOPs:\t38.8\n"
     ]
    }
   ],
   "source": [
    "mbert_3 = mbert_1\n",
    "mbert_3.deepspeed = True\n",
    "mbert_3.ds_config = 'transformers/tests/deepspeed/ds_config_zero2.json'\n",
    "mbert_3.batch_size = 128\n",
    "run_bert(mbert_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "de6b1cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total used GPU memory:\t22.4 GB\n",
      "# samples per second:\t278.7\n",
      "Measured per GPU TFLOPs:\t36.3\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "run_bert(mbert_3)\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "676dbe19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total used GPU memory:\t16.2 GB\n",
      "# samples per second:\t7.0\n",
      "Measured per GPU TFLOPs:\t18.8\n"
     ]
    }
   ],
   "source": [
    "mgpt_1 = Config(\"gpt2-large\", 1024, 2, bf16=True, optim='adamw_apex_fused', \n",
    "                grad_accum=16, gpus='0,1', steps=5, deepspeed=True, \n",
    "                ds_config='transformers/tests/deepspeed/ds_config_zero2.json')\n",
    "run_gpt(mgpt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "879f1242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total used GPU memory:\t16.2 GB\n",
      "# samples per second:\t5.7\n",
      "Measured per GPU TFLOPs:\t15.0\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "run_gpt(mgpt_1)\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a19505fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total used GPU memory:\t15.3 GB\n",
      "# samples per second:\t2.2\n",
      "Measured per GPU TFLOPs:\t11.8\n"
     ]
    }
   ],
   "source": [
    "mgpt_2 = Config(\"gpt2-xl\", 1024, 1, bf16=True, optim='adamw_apex_fused', \n",
    "                grad_accum=16, gpus='0,1', deepspeed=True, steps=5,\n",
    "                ds_config='transformers/tests/deepspeed/ds_config_zero2.json')\n",
    "run_gpt(mgpt_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a8f6f9",
   "metadata": {},
   "source": [
    "This is what"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
